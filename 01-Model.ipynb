{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0f2d67",
   "metadata": {},
   "source": [
    "# QAnon Reddit NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05207d36-4621-4acf-9698-e6159f8f20db",
   "metadata": {},
   "source": [
    "Name: Mateusz Kolodziejczyk\n",
    "\n",
    "Student Number: 20084190"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e358e",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c9400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "pd.set_option('display.max_columns', None)  \n",
    "\n",
    "DEBUG = False\n",
    "SEED = 666\n",
    "\n",
    "import os\n",
    "for d in ['orig','data','output']: os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b9846",
   "metadata": {},
   "source": [
    "## Setup, load and prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f8818-9804-41b9-b68f-2a25153fe2e9",
   "metadata": {},
   "source": [
    "#### Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95714e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle file ... (13182, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>isUQ</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa65b7dd5d5fa660d058e094669f884bf7d52299</td>\n",
       "      <td>0</td>\n",
       "      <td>Active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2b1505f289338751829dfa129c0b52d145c9eceb</td>\n",
       "      <td>1</td>\n",
       "      <td>Active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4eeddb9abeb3c4889f1b037016bf2aeb834bb66d</td>\n",
       "      <td>0</td>\n",
       "      <td>Active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08a6fae5a56fcdb495b2de8a02625ea2b4abe32f</td>\n",
       "      <td>1</td>\n",
       "      <td>Active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01301652214982c57efe894efe7e2c7d57df2801</td>\n",
       "      <td>0</td>\n",
       "      <td>Active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     author  isUQ  status\n",
       "0  aa65b7dd5d5fa660d058e094669f884bf7d52299     0  Active\n",
       "1  2b1505f289338751829dfa129c0b52d145c9eceb     1  Active\n",
       "2  4eeddb9abeb3c4889f1b037016bf2aeb834bb66d     0  Active\n",
       "3  08a6fae5a56fcdb495b2de8a02625ea2b4abe32f     1  Active\n",
       "4  01301652214982c57efe894efe7e2c7d57df2801     0  Active"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename = \"authors\"\n",
    "\n",
    "if os.path.isfile(f\"data/{basename}.pickle\"):\n",
    "    # loading\n",
    "    print(\"Reading pickle file\", end=\" ... \")\n",
    "    df_authors = pd.read_pickle(f\"data/{basename}.pickle\")\n",
    "else:\n",
    "    # loading\n",
    "    print(\"Reading csv\", end=\" ... \")\n",
    "    df_authors = pd.read_csv(f\"orig/{basename}.csv.gz\")\n",
    "    # cleaning\n",
    "    print(df_authors.shape, end=\" ... \")\n",
    "    df_authors.rename(columns={\"QAuthor\":\"author\"}, inplace=True)\n",
    "    # save as pickle for later use\n",
    "    print(\"generating pickle file\", end=\" ... \")\n",
    "    df_authors.to_pickle(f\"data/{basename}.pickle\")\n",
    "\n",
    "print(df_authors.shape)\n",
    "df_authors.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837d936-cc5a-42cc-829c-8a85b21b8012",
   "metadata": {},
   "source": [
    "#### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a18cfd-0416-488c-80a1-a8d14256c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle file ... (10831841, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>date_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e0mztbn</td>\n",
       "      <td>t3_8qy7gp</td>\n",
       "      <td>t3_8qy7gp</td>\n",
       "      <td>182c774799aac38a84f5117fc59cde99b0df19af</td>\n",
       "      <td>greatawakening</td>\n",
       "      <td>My account is new because i lost my password t...</td>\n",
       "      <td>2018-06-14 02:17:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e0n0e9q</td>\n",
       "      <td>t3_8qy9wy</td>\n",
       "      <td>t3_8qy9wy</td>\n",
       "      <td>182c774799aac38a84f5117fc59cde99b0df19af</td>\n",
       "      <td>greatawakening</td>\n",
       "      <td>new account only because i lost the password t...</td>\n",
       "      <td>2018-06-14 02:28:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e0n11j4</td>\n",
       "      <td>t3_8qy9wy</td>\n",
       "      <td>t3_8qy9wy</td>\n",
       "      <td>182c774799aac38a84f5117fc59cde99b0df19af</td>\n",
       "      <td>greatawakening</td>\n",
       "      <td>i appreciate all the comments i'll read thru t...</td>\n",
       "      <td>2018-06-14 02:40:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e0n1q4v</td>\n",
       "      <td>t3_8qy9wy</td>\n",
       "      <td>t1_e0n0vfm</td>\n",
       "      <td>182c774799aac38a84f5117fc59cde99b0df19af</td>\n",
       "      <td>greatawakening</td>\n",
       "      <td>why would rosenstein threaten those asking (li...</td>\n",
       "      <td>2018-06-14 02:52:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e0n1u7v</td>\n",
       "      <td>t3_8qy9wy</td>\n",
       "      <td>t1_e0n0ltk</td>\n",
       "      <td>182c774799aac38a84f5117fc59cde99b0df19af</td>\n",
       "      <td>greatawakening</td>\n",
       "      <td>interesting i'm reading now. i'm still confuse...</td>\n",
       "      <td>2018-06-14 02:54:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    link_id   parent_id                                    author  \\\n",
       "0  e0mztbn  t3_8qy7gp   t3_8qy7gp  182c774799aac38a84f5117fc59cde99b0df19af   \n",
       "1  e0n0e9q  t3_8qy9wy   t3_8qy9wy  182c774799aac38a84f5117fc59cde99b0df19af   \n",
       "2  e0n11j4  t3_8qy9wy   t3_8qy9wy  182c774799aac38a84f5117fc59cde99b0df19af   \n",
       "3  e0n1q4v  t3_8qy9wy  t1_e0n0vfm  182c774799aac38a84f5117fc59cde99b0df19af   \n",
       "4  e0n1u7v  t3_8qy9wy  t1_e0n0ltk  182c774799aac38a84f5117fc59cde99b0df19af   \n",
       "\n",
       "        subreddit                                               body  \\\n",
       "0  greatawakening  My account is new because i lost my password t...   \n",
       "1  greatawakening  new account only because i lost the password t...   \n",
       "2  greatawakening  i appreciate all the comments i'll read thru t...   \n",
       "3  greatawakening  why would rosenstein threaten those asking (li...   \n",
       "4  greatawakening  interesting i'm reading now. i'm still confuse...   \n",
       "\n",
       "         date_created  \n",
       "0 2018-06-14 02:17:37  \n",
       "1 2018-06-14 02:28:21  \n",
       "2 2018-06-14 02:40:09  \n",
       "3 2018-06-14 02:52:41  \n",
       "4 2018-06-14 02:54:46  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename = \"comments\"\n",
    "\n",
    "if os.path.isfile(f\"data/{basename}.pickle\"):\n",
    "    # loading\n",
    "    print(\"Reading pickle file\", end=\" ... \")\n",
    "    df_comments = pd.read_pickle(f\"data/{basename}.pickle\")\n",
    "else:\n",
    "    # loading\n",
    "    print(\"Reading csv\", end=\" ... \")\n",
    "    df_comments = pd.read_csv(f\"orig/{basename}.csv.gz\", dtype=str, parse_dates=[\"date_created\"])\n",
    "    # cleaning\n",
    "    print(df_comments.shape, \"dropna\", end=\" ... \")\n",
    "    df_comments.dropna(inplace=True)\n",
    "    print(df_comments.shape, end=\" ... \")\n",
    "    # save as pickle for later use\n",
    "    print(\"generating pickle file\", end=\" ... \")\n",
    "    df_comments.to_pickle(f\"data/{basename}.pickle\")\n",
    "\n",
    "print(df_comments.shape)\n",
    "df_comments.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba0f66f-60ac-451b-bad7-922a707ed062",
   "metadata": {},
   "source": [
    "#### Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdace7db-8673-4683-89aa-649ece81981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle file ... (2099686, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>numReplies</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>is_self</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>date_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>8xuv4i</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>I’ve been writing “Trump Lives Here” on my $20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://i.redd.it/h3mbbxvxq7911.jpg</td>\n",
       "      <td>/r/greatawakening/comments/8xuv4i/ive_been_wri...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-11 00:27:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>8ydw3e</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>Trying to take him seriously but...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://i.redd.it/62gaw0th4l911.jpg</td>\n",
       "      <td>/r/greatawakening/comments/8ydw3e/trying_to_ta...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-12 21:26:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>8ytwg0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>“It is all happening!” Crumb?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://i.redd.it/yo9zscb1jx911.jpg</td>\n",
       "      <td>/r/greatawakening/comments/8ytwg0/it_is_all_ha...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-14 15:09:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>8ytx4z</td>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>“It is all happening!” Positive sign hopefully...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>https://i.redd.it/v5c4zxcjjx911.jpg</td>\n",
       "      <td>/r/greatawakening/comments/8ytx4z/it_is_all_ha...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-14 15:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>8yvgwt</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>Pedogate is REAL! Happening here in my beloved...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>http://www.foxnews.com/us/2018/07/14/texas-wom...</td>\n",
       "      <td>/r/greatawakening/comments/8yvgwt/pedogate_is_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-14 18:46:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subreddit      id score numReplies  \\\n",
       "0  greatawakening  8xuv4i     1         14   \n",
       "1  greatawakening  8ydw3e     1         13   \n",
       "2  greatawakening  8ytwg0     1          0   \n",
       "3  greatawakening  8ytx4z     1        114   \n",
       "4  greatawakening  8yvgwt     1         23   \n",
       "\n",
       "                                     author  \\\n",
       "0  879f283b831c13474e219e88663d95b0763cca9b   \n",
       "1  879f283b831c13474e219e88663d95b0763cca9b   \n",
       "2  879f283b831c13474e219e88663d95b0763cca9b   \n",
       "3  879f283b831c13474e219e88663d95b0763cca9b   \n",
       "4  879f283b831c13474e219e88663d95b0763cca9b   \n",
       "\n",
       "                                               title text is_self  \\\n",
       "0  I’ve been writing “Trump Lives Here” on my $20...  NaN   False   \n",
       "1                Trying to take him seriously but...  NaN   False   \n",
       "2                      “It is all happening!” Crumb?  NaN   False   \n",
       "3  “It is all happening!” Positive sign hopefully...  NaN   False   \n",
       "4  Pedogate is REAL! Happening here in my beloved...  NaN   False   \n",
       "\n",
       "        domain                                                url  \\\n",
       "0    i.redd.it                https://i.redd.it/h3mbbxvxq7911.jpg   \n",
       "1    i.redd.it                https://i.redd.it/62gaw0th4l911.jpg   \n",
       "2    i.redd.it                https://i.redd.it/yo9zscb1jx911.jpg   \n",
       "3    i.redd.it                https://i.redd.it/v5c4zxcjjx911.jpg   \n",
       "4  foxnews.com  http://www.foxnews.com/us/2018/07/14/texas-wom...   \n",
       "\n",
       "                                           permalink  upvote_ratio  \\\n",
       "0  /r/greatawakening/comments/8xuv4i/ive_been_wri...          -1.0   \n",
       "1  /r/greatawakening/comments/8ydw3e/trying_to_ta...          -1.0   \n",
       "2  /r/greatawakening/comments/8ytwg0/it_is_all_ha...          -1.0   \n",
       "3  /r/greatawakening/comments/8ytx4z/it_is_all_ha...          -1.0   \n",
       "4  /r/greatawakening/comments/8yvgwt/pedogate_is_...          -1.0   \n",
       "\n",
       "         date_created  \n",
       "0 2018-07-11 00:27:24  \n",
       "1 2018-07-12 21:26:32  \n",
       "2 2018-07-14 15:09:25  \n",
       "3 2018-07-14 15:12:14  \n",
       "4 2018-07-14 18:46:35  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename = \"submissions\"\n",
    "\n",
    "if os.path.isfile(f\"data/{basename}.pickle\"):\n",
    "    # loading\n",
    "    print(\"Reading pickle file\", end=\" ... \")\n",
    "    df_submissions = pd.read_pickle(f\"data/{basename}.pickle\")\n",
    "else:\n",
    "    # loading\n",
    "    print(\"Reading csv\", end=\" ... \")\n",
    "    df_submissions = pd.read_csv(f\"orig/{basename}.csv.gz\", parse_dates=[\"date_created\"])\n",
    "    subset_labels = df_submissions.drop(['text'], axis=1).columns.values.tolist()\n",
    "    # cleaning\n",
    "    print(df_submissions.shape, \"dropna\", end=\" ... \")\n",
    "    \n",
    "    # Make sure text is ignored when dropping na\n",
    "    df_submissions.dropna(inplace=True, subset=subset_labels)\n",
    "\n",
    "    print(df_submissions.shape, end=\" ... \")\n",
    "    # save as pickle for later use\n",
    "    print(\"generating pickle file\", end=\" ... \")\n",
    "    df_submissions.to_pickle(f\"data/{basename}.pickle\")\n",
    "\n",
    "print(df_submissions.shape)\n",
    "df_submissions.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbae02-ddc6-4ba7-b615-b387b13bfb2b",
   "metadata": {},
   "source": [
    "#### Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa3b6e2-8794-4fb3-aba8-812281164dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle file ... (12987, 38)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>numSubscribers</th>\n",
       "      <th>status</th>\n",
       "      <th>allModNames</th>\n",
       "      <th>allMods</th>\n",
       "      <th>qModNames</th>\n",
       "      <th>qMods</th>\n",
       "      <th>top_qModNames</th>\n",
       "      <th>top_qMods</th>\n",
       "      <th>firstPostSubmission</th>\n",
       "      <th>lastPostSubmission</th>\n",
       "      <th>firstPostComment</th>\n",
       "      <th>lastPostComment</th>\n",
       "      <th>qModsRatio</th>\n",
       "      <th>top_qModsRatio</th>\n",
       "      <th>activePreBanOnly</th>\n",
       "      <th>activePreQ</th>\n",
       "      <th>activePostBan</th>\n",
       "      <th>qAuth</th>\n",
       "      <th>top_qAuth</th>\n",
       "      <th>qSubmissions</th>\n",
       "      <th>top_qSubmissions</th>\n",
       "      <th>nonTop_qSubmissions</th>\n",
       "      <th>qComments</th>\n",
       "      <th>top_qComments</th>\n",
       "      <th>nonTop_qComments</th>\n",
       "      <th>top_qPercent</th>\n",
       "      <th>qPercent</th>\n",
       "      <th>Monthly Average Total Authors</th>\n",
       "      <th>Monthly Average Total Submissions</th>\n",
       "      <th>Monthly Average UQ Authors</th>\n",
       "      <th>Monthly Average UQ Submissions</th>\n",
       "      <th>Monthly Average QAnon Authors</th>\n",
       "      <th>Monthly Average QAnon Submissions</th>\n",
       "      <th>% UQ Submissions</th>\n",
       "      <th>% UQ Authors</th>\n",
       "      <th>% QAnon Submissions</th>\n",
       "      <th>% QAnon Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Watches</td>\n",
       "      <td>1525243.0</td>\n",
       "      <td>public</td>\n",
       "      <td>['f1c355408b78fd88ebc13aade4c9a7924005c2ab', '...</td>\n",
       "      <td>13</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-12-07 03:21:16</td>\n",
       "      <td>2020-11-27 19:24:49</td>\n",
       "      <td>2016-11-26 13:24:54</td>\n",
       "      <td>2021-01-22 18:22:43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>219</td>\n",
       "      <td>99</td>\n",
       "      <td>120</td>\n",
       "      <td>1681.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2881.384615</td>\n",
       "      <td>5911.846154</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.040596</td>\n",
       "      <td>0.041647</td>\n",
       "      <td>0.101491</td>\n",
       "      <td>0.130146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MMA</td>\n",
       "      <td>1518451.0</td>\n",
       "      <td>public</td>\n",
       "      <td>['69e403df92bb49af60d5046c0be60f1a46bfd53d', '...</td>\n",
       "      <td>20</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-11-01 05:51:55</td>\n",
       "      <td>2021-01-03 00:43:44</td>\n",
       "      <td>2016-10-28 00:39:15</td>\n",
       "      <td>2021-01-23 08:16:57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>29</td>\n",
       "      <td>371</td>\n",
       "      <td>99</td>\n",
       "      <td>272</td>\n",
       "      <td>18910.0</td>\n",
       "      <td>2808.0</td>\n",
       "      <td>16102.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1840.461538</td>\n",
       "      <td>4953.461538</td>\n",
       "      <td>2.916667</td>\n",
       "      <td>4.75</td>\n",
       "      <td>6.384615</td>\n",
       "      <td>9.923077</td>\n",
       "      <td>0.095893</td>\n",
       "      <td>0.158475</td>\n",
       "      <td>0.200326</td>\n",
       "      <td>0.346903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>281450.0</td>\n",
       "      <td>public</td>\n",
       "      <td>['14a0b21d55a0415e5541be3389d27ee3b3232c90', '...</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-11-07 07:12:42</td>\n",
       "      <td>2021-01-18 23:13:14</td>\n",
       "      <td>2016-11-03 16:45:54</td>\n",
       "      <td>2021-01-21 14:56:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>188</td>\n",
       "      <td>99</td>\n",
       "      <td>89</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.40</td>\n",
       "      <td>732.769231</td>\n",
       "      <td>1140.692308</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.272727</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>0.105199</td>\n",
       "      <td>0.163762</td>\n",
       "      <td>0.334725</td>\n",
       "      <td>0.446625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnusAnnus</td>\n",
       "      <td>195890.0</td>\n",
       "      <td>quarantined/private</td>\n",
       "      <td>['f7fd9c68f804acda665d2ab082217bb1583318f2']</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-05 05:04:36</td>\n",
       "      <td>2020-11-14 05:24:35</td>\n",
       "      <td>2020-06-17 23:26:32</td>\n",
       "      <td>2020-11-13 09:04:06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GlobalOffensiveTrade</td>\n",
       "      <td>209007.0</td>\n",
       "      <td>public</td>\n",
       "      <td>['f78b47357eabba6f7580e47da7560a322287eaee', '...</td>\n",
       "      <td>22</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-12-02 06:33:48</td>\n",
       "      <td>2020-08-30 21:38:25</td>\n",
       "      <td>2017-04-05 12:44:22</td>\n",
       "      <td>2019-12-19 01:06:07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>99</td>\n",
       "      <td>23</td>\n",
       "      <td>86.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2132.923077</td>\n",
       "      <td>17479.307692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.046884</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.046884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit  numSubscribers               status  \\\n",
       "0               Watches       1525243.0               public   \n",
       "1                   MMA       1518451.0               public   \n",
       "2               Seattle        281450.0               public   \n",
       "3             UnusAnnus        195890.0  quarantined/private   \n",
       "4  GlobalOffensiveTrade        209007.0               public   \n",
       "\n",
       "                                         allModNames  allMods qModNames  \\\n",
       "0  ['f1c355408b78fd88ebc13aade4c9a7924005c2ab', '...       13        []   \n",
       "1  ['69e403df92bb49af60d5046c0be60f1a46bfd53d', '...       20        []   \n",
       "2  ['14a0b21d55a0415e5541be3389d27ee3b3232c90', '...        7        []   \n",
       "3       ['f7fd9c68f804acda665d2ab082217bb1583318f2']        1        []   \n",
       "4  ['f78b47357eabba6f7580e47da7560a322287eaee', '...       22        []   \n",
       "\n",
       "   qMods top_qModNames  top_qMods  firstPostSubmission   lastPostSubmission  \\\n",
       "0    0.0            []        0.0  2016-12-07 03:21:16  2020-11-27 19:24:49   \n",
       "1    0.0            []        0.0  2016-11-01 05:51:55  2021-01-03 00:43:44   \n",
       "2    0.0            []        0.0  2016-11-07 07:12:42  2021-01-18 23:13:14   \n",
       "3    0.0            []        0.0  2020-02-05 05:04:36  2020-11-14 05:24:35   \n",
       "4    0.0            []        0.0  2016-12-02 06:33:48  2020-08-30 21:38:25   \n",
       "\n",
       "      firstPostComment      lastPostComment  qModsRatio  top_qModsRatio  \\\n",
       "0  2016-11-26 13:24:54  2021-01-22 18:22:43         0.0             0.0   \n",
       "1  2016-10-28 00:39:15  2021-01-23 08:16:57         0.0             0.0   \n",
       "2  2016-11-03 16:45:54  2021-01-21 14:56:00         0.0             0.0   \n",
       "3  2020-06-17 23:26:32  2020-11-13 09:04:06         0.0             0.0   \n",
       "4  2017-04-05 12:44:22  2019-12-19 01:06:07         0.0             0.0   \n",
       "\n",
       "   activePreBanOnly  activePreQ  activePostBan  qAuth  top_qAuth  \\\n",
       "0                 0           1              1     58         10   \n",
       "1                 0           1              1     97         29   \n",
       "2                 0           1              1     53         13   \n",
       "3                 0           0              1      1          1   \n",
       "4                 0           1              1      6          1   \n",
       "\n",
       "   qSubmissions  top_qSubmissions  nonTop_qSubmissions  qComments  \\\n",
       "0           219                99                  120     1681.0   \n",
       "1           371                99                  272    18910.0   \n",
       "2           188                99                   89     1559.0   \n",
       "3            99                99                    0        3.0   \n",
       "4           122                99                   23       86.0   \n",
       "\n",
       "   top_qComments  nonTop_qComments  top_qPercent  qPercent  \\\n",
       "0          244.0            1437.0          0.29      0.44   \n",
       "1         2808.0           16102.0          0.83      0.74   \n",
       "2          214.0            1345.0          0.37      0.40   \n",
       "3            0.0               0.0          0.03      0.01   \n",
       "4           46.0              40.0          0.03      0.05   \n",
       "\n",
       "   Monthly Average Total Authors  Monthly Average Total Submissions  \\\n",
       "0                    2881.384615                        5911.846154   \n",
       "1                    1840.461538                        4953.461538   \n",
       "2                     732.769231                        1140.692308   \n",
       "3                       0.000000                           0.000000   \n",
       "4                    2132.923077                       17479.307692   \n",
       "\n",
       "   Monthly Average UQ Authors  Monthly Average UQ Submissions  \\\n",
       "0                    1.200000                            2.40   \n",
       "1                    2.916667                            4.75   \n",
       "2                    1.200000                            1.20   \n",
       "3                    0.000000                            0.00   \n",
       "4                    1.000000                           19.00   \n",
       "\n",
       "   Monthly Average QAnon Authors  Monthly Average QAnon Submissions  \\\n",
       "0                       3.750000                           6.000000   \n",
       "1                       6.384615                           9.923077   \n",
       "2                       3.272727                           3.818182   \n",
       "3                       0.000000                           0.000000   \n",
       "4                       1.000000                          19.000000   \n",
       "\n",
       "   % UQ Submissions  % UQ Authors  % QAnon Submissions  % QAnon Authors  \n",
       "0          0.040596      0.041647             0.101491         0.130146  \n",
       "1          0.095893      0.158475             0.200326         0.346903  \n",
       "2          0.105199      0.163762             0.334725         0.446625  \n",
       "3          0.000000      0.000000             0.000000         0.000000  \n",
       "4          0.108700      0.046884             0.108700         0.046884  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename = \"subreddits\"\n",
    "\n",
    "if os.path.isfile(f\"data/{basename}.pickle\"):\n",
    "    # loading\n",
    "    print(\"Reading pickle file\", end=\" ... \")\n",
    "    df_subreddits = pd.read_pickle(f\"data/{basename}.pickle\")\n",
    "else:\n",
    "    # loading\n",
    "    print(\"Reading csv\", end=\" ... \")\n",
    "    df_subreddits = pd.read_csv(f\"orig/{basename}.csv.gz\", converters={\"allModNames\": lambda x: x.strip(\"[]\").split(\",\")})\n",
    "    # cleaning\n",
    "    print(df_subreddits.shape, \"dropna\", end=\" ... \")\n",
    "    \n",
    "    # Make sure text is ignored when dropping na\n",
    "    df_subreddits.dropna(inplace=True)\n",
    "    print(df_subreddits.shape, end=\" ... \")\n",
    "    # save as pickle for later use\n",
    "    print(\"generating pickle file\", end=\" ... \")\n",
    "    df_subreddits.to_pickle(f\"data/{basename}.pickle\")\n",
    "\n",
    "print(df_subreddits.shape)\n",
    "df_subreddits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47dc2c-9f34-4e2d-a9ce-1fa4c7cff98d",
   "metadata": {},
   "source": [
    "#### Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050d3431-06a9-403d-8109-1d33acd5b78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pickle file ... (19, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greatawakening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The_GreatAwakening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFTERTHESTQRM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheGreatAwakening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QAnon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit\n",
       "0      greatawakening\n",
       "1  The_GreatAwakening\n",
       "2       AFTERTHESTQRM\n",
       "3   TheGreatAwakening\n",
       "4               QAnon"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename = \"paper\"\n",
    "\n",
    "if os.path.isfile(f\"data/{basename}.pickle\"):\n",
    "    # loading\n",
    "    print(\"Reading pickle file\", end=\" ... \")\n",
    "    df_paper = pd.read_pickle(f\"data/{basename}.pickle\")\n",
    "else:\n",
    "    # loading\n",
    "    print(\"Reading csv\", end=\" ... \")\n",
    "    df_paper = pd.read_csv(f\"orig/{basename}.csv\", dtype=str)\n",
    "    # cleaning\n",
    "    print(df_paper.shape, \"dropna\", end=\" ... \")\n",
    "    \n",
    "    # Make sure text is ignored when dropping na\n",
    "    df_paper.dropna(inplace=True)\n",
    "    print(df_paper.shape, end=\" ... \")\n",
    "    # save as pickle for later use\n",
    "    print(\"generating pickle file\", end=\" ... \")\n",
    "    df_paper.to_pickle(f\"data/{basename}.pickle\")\n",
    "\n",
    "print(df_paper.shape)\n",
    "df_paper.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cec8ea-d6ba-4841-82b6-f613c2eb6905",
   "metadata": {},
   "source": [
    "## Verify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3617305a-fc73-471f-a94d-d5b663c3cdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_authors.author is unique: True\n",
      "no missing values in df_authors: True\n",
      "df_comments.id is unique id: True\n",
      "each comment author is in list of qanon authors: True\n",
      "df_submissions.id is a unique id: True\n",
      "each submission author is in list of qanon authors: True\n",
      "in df_subreddits, the number of moderators in allmodnames matches count in allmods: False\n"
     ]
    }
   ],
   "source": [
    "n_test = 7\n",
    "n_true = 0\n",
    "# df_authors.author is a uniqueid\n",
    "\n",
    "val = df_authors['author'].is_unique\n",
    "print(\"df_authors.author is unique: {}\".format(val))\n",
    "if val == True:\n",
    "    n_true += 1\n",
    "# no missing values in df_authors\n",
    "\n",
    "val = df_authors.notna().any().any()\n",
    "print(\"no missing values in df_authors: {}\".format(val))\n",
    "if val == True:\n",
    "    n_true += 1\n",
    "# df_comments.id is unique id\n",
    "\n",
    "val = df_comments['id'].is_unique\n",
    "print(\"df_comments.id is unique id: {}\".format(val))\n",
    "if val == True:\n",
    "    n_true += 1\n",
    "# each comment author is in list of qanon authors\n",
    "\n",
    "val = df_comments['author'].isin(df_authors['author']).all()\n",
    "print(\"each comment author is in list of qanon authors: {}\".format(val))\n",
    "if val == True:\n",
    "    n_true += 1\n",
    "    \n",
    "# df_submissions.id is a unique id\n",
    "\n",
    "val = df_submissions['id'].is_unique\n",
    "print(\"df_submissions.id is a unique id: {}\".format(val))\n",
    "if val == True:\n",
    "    n_true += 1\n",
    "    \n",
    "# each submission author is in list of qanon authors\n",
    "val = df_submissions['author'].isin(df_authors['author']).all()\n",
    "print(\"each submission author is in list of qanon authors: {}\".format(val))\n",
    "if val == True:\n",
    "    n_true += 1\n",
    "    \n",
    "# in df_subreddits, the number of moderates in allmodnames matches count in allmods\n",
    "val = np.where(df_subreddits['allModNames'].str.len() == df_subreddits['allMods'], True, False).all()\n",
    "print(\"in df_subreddits, the number of moderators in allmodnames matches count in allmods: {}\".format(val))\n",
    "if val == True:\n",
    "    n_true += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75eb767c-5308-47cc-92da-1efd5fa409ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/7 verification tests passed\n"
     ]
    }
   ],
   "source": [
    "print(\"{}/{} verification tests passed\".format(n_true, n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b27473-abb1-4688-a5bd-4bb19bc4973c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([  425, 12562], dtype=int64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_count = np.where(df_subreddits['allModNames'].str.len() == df_subreddits['allMods'], True, False)\n",
    "np.unique(match_count, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad4f7aa-49a2-44c6-9e46-30373c9e43d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference = np.absolute(df_subreddits['allModNames'].str.len() - df_subreddits['allMods'])\n",
    "difference.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dfdc17-d8d4-4925-8ce9-af115f508857",
   "metadata": {},
   "source": [
    "One verification test failed, but most values match. Furthermore if the values don't match, they're only off by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f2479-6ccb-4b58-acf9-f10afbc20e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44f8112e-a5cd-43c2-bcaa-d1fbacb582a1",
   "metadata": {},
   "source": [
    "## Match Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc05fa-54fe-4e43-b4d1-9373a73a2d5f",
   "metadata": {},
   "source": [
    "### Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8711555-b50f-4965-8f91-774c50623d06",
   "metadata": {},
   "source": [
    "Get QAnon enthusiastic (top 25% most active with five or more submissions to the 19 QANON subreddits) Should be 3506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3aacb08-427e-425d-a75b-103f1cbc3f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Active</th>\n",
       "      <th>Does Not Exist</th>\n",
       "      <th>Suspended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q_interested</th>\n",
       "      <td>9583</td>\n",
       "      <td>8007</td>\n",
       "      <td>1143</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q_enthusiastic</th>\n",
       "      <td>3599</td>\n",
       "      <td>3007</td>\n",
       "      <td>481</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Total  Active  Does Not Exist  Suspended\n",
       "q_interested     9583    8007            1143        433\n",
       "q_enthusiastic   3599    3007             481        111"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove all non Q subreddits from submissions\n",
    "filtered_submissions = df_submissions[df_submissions['subreddit'].isin(df_paper['subreddit'])]\n",
    "authors = filtered_submissions['author']\n",
    "author_counts = authors.value_counts()\n",
    "cutoff = 5\n",
    "    \n",
    "\n",
    "q_enthusiastic_counts = author_counts[author_counts >= cutoff]\n",
    "q_interested_counts = author_counts[author_counts < cutoff]\n",
    "\n",
    "q_enthusiastic = q_enthusiastic_counts.index.to_series()\n",
    "q_interested = q_interested_counts.index.to_series()\n",
    "\n",
    "indexes = ['q_interested', 'q_enthusiastic']\n",
    "q_users={indexes[0]: q_interested, indexes[1]: q_enthusiastic}\n",
    "\n",
    "\n",
    "table1 = {indexes[0]: {\"Total\":0, \"Active\": 0, \"Does Not Exist\": 0, \"Suspended\": 0},\n",
    "                   indexes[1]: {\"Total\":0, \"Active\": 0, \"Does Not Exist\": 0, \"Suspended\": 0}  }\n",
    "\n",
    "table1[indexes[0]][\"Total\"] = q_users[indexes[0]].count()\n",
    "\n",
    "for k in q_users:\n",
    "    d = table1[k]\n",
    "    users = q_users[k]\n",
    "    d[\"Total\"] = users.count()\n",
    "    account_statuses = df_authors[df_authors['author'].isin(users)]['status'].value_counts()\n",
    "    d[\"Active\"] = account_statuses.at['Active']\n",
    "    d[\"Does Not Exist\"] = account_statuses.at['DNE']\n",
    "    d[\"Suspended\"] = account_statuses.at['Is_suspended']\n",
    "\n",
    "    \n",
    "pd.DataFrame.from_dict(table1).transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae3dc1-404c-437a-af83-c259d7f89be6",
   "metadata": {},
   "source": [
    "### Table 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ccea6e-48b5-4784-94d5-d56c65723f5b",
   "metadata": {},
   "source": [
    "Iterate through each suberddit, get date and populate dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36689dc0-58f6-4c40-b0a7-de15abc4e5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submissions_authors</th>\n",
       "      <th>submissions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>greatawakening</th>\n",
       "      <td>12862</td>\n",
       "      <td>95644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The_GreatAwakening</th>\n",
       "      <td>698</td>\n",
       "      <td>3982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFTERTHESTQRM</th>\n",
       "      <td>46</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheGreatAwakening</th>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QAnon</th>\n",
       "      <td>59</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QGreatAwakening</th>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QanonUK</th>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QanonTools</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greatawakening2</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quincels</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WWG1WGA01</th>\n",
       "      <td>18</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TheCalmBeforeTheStorm</th>\n",
       "      <td>306</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BiblicalQ</th>\n",
       "      <td>142</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thestorm</th>\n",
       "      <td>33</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QProofs</th>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QanonandJFKjr</th>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qresearch</th>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NorthernAwakening</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4U</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       submissions_authors  submissions\n",
       "greatawakening                       12862        95644\n",
       "The_GreatAwakening                     698         3982\n",
       "AFTERTHESTQRM                           46          730\n",
       "TheGreatAwakening                       12           32\n",
       "QAnon                                   59          119\n",
       "QGreatAwakening                         23           34\n",
       "QanonUK                                 12           27\n",
       "QanonTools                               3           18\n",
       "greatawakening2                          6            7\n",
       "Quincels                                 4            5\n",
       "WWG1WGA01                               18          207\n",
       "TheCalmBeforeTheStorm                  306         1145\n",
       "BiblicalQ                              142          463\n",
       "thestorm                                33          236\n",
       "QProofs                                  2           79\n",
       "QanonandJFKjr                           14           31\n",
       "qresearch                               15           19\n",
       "NorthernAwakening                        5           13\n",
       "Q4U                                      2            5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2 = {}\n",
    "for sub in df_paper['subreddit'].values:\n",
    "    # All submission rows for this subreddit\n",
    "    submissions = df_submissions[df_submissions['subreddit'] == sub]\n",
    "    # Get number of submission authors\n",
    "    author_count = submissions['author'].value_counts().count()\n",
    "    # Get number of submissions\n",
    "    submission_count = submissions['id'].count()\n",
    "    table2[sub] = {\"submissions_authors\": author_count, \"submissions\": submission_count}\n",
    "\n",
    "table2\n",
    "\n",
    "pd.DataFrame.from_dict(table2).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd11aa-26c1-4f9b-9b8a-2da6846c1b46",
   "metadata": {},
   "source": [
    "## Text Preprocessingimport emoji, string, nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1738d0dc-df09-4119-a4fd-7a22d0263cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji, string, nltk\n",
    "import spacy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "from nltk.util import ngrams\n",
    "from gensim.models import Phrases\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "581c74ba-13f2-4006-932e-5f6191da5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = [\n",
    "    \"The movie was crap \",\n",
    "    \"The book was kind of good.\",\n",
    "    \"A really bad, horrible book. 🤨\", \n",
    "    \"Today sux\", \n",
    "    \"12 Today kinda sux! But I'll get by, lol 😃\",\n",
    "    \"sentiment analysis is shit. \",\n",
    "    \"sentiment analysis is the shit.\",\n",
    "    \"I like to hate Michael Bay films, but I couldn't fault this one\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ebeb38d-0ef2-4778-aff2-3a0b5cda927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define three possible tokenizers\n",
    "tokenizer_1 = str.split\n",
    "tokenizer_2 = nltk.word_tokenize\n",
    "tokenizer_3 = RegexpTokenizer(\"\\w+|\\$[\\d\\.]+|http\\S+\").tokenize\n",
    "\n",
    "# define two possible default_stopwords\n",
    "default_stopwords_1 = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "default_stopwords_2 = nlp.Defaults.stop_words\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f7806c-4f3f-494a-95fc-80a8e4b46669",
   "metadata": {},
   "outputs": [],
   "source": [
    "annoying_quotes = ['“', '”']\n",
    "def generate_tokens(text, lower=True, \n",
    "    tokenizer=None,\n",
    "    default_stopwords=None, extra_stopwords=None, \n",
    "    lemmatizer=None, stemmer=None,\n",
    "    min_token_length=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a string, separate into tokens and clean.\n",
    "    \"\"\"\n",
    "    # lower case\n",
    "    text = text.lower() if lower else text \n",
    "\n",
    "    # remove emoji \n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    # tokenize text (always needed)\n",
    "    assert tokenizer is not None, \"Need to specify a tokenizer to split text into tokens\"\n",
    "    tokens = tokenizer(text)\n",
    "\n",
    "    # TODO drop default_stopwords and extra_stopwords (if parameters are provided) and punctuation\n",
    "    if default_stopwords is not None:\n",
    "        [t for t in tokens if t not in default_stopwords]\n",
    "    if extra_stopwords is not None:\n",
    "        [t for t in tokens if t not in extra_stopwords]\n",
    "        \n",
    "    tokens = [t for t in tokens if t not in string.punctuation and t not in annoying_quotes]\n",
    "    \n",
    "    # TODO lemmatize each token (only if lemmatizer parameter is provided)\n",
    "    if lemmatizer is not None:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        \n",
    "    # TODO stem each token (only if stemmer parameter is provided)\n",
    "    if stemmer is not None:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        \n",
    "    if min_token_length>0:\n",
    "        tokens = [t for t in tokens if len(t)>=min_token_length]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4be56-75ac-463a-b09c-c78ea033b52e",
   "metadata": {},
   "source": [
    "Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6eb3525-7733-4b7c-9c8c-79140209e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was crap \n",
      "\t['the', 'movie', 'wa', 'crap']\n",
      "The book was kind of good.\n",
      "\t['the', 'book', 'wa', 'kind', 'of', 'good']\n",
      "A really bad, horrible book. 🤨\n",
      "\t['a', 'really', 'bad', 'horrible', 'book']\n",
      "Today sux\n",
      "\t['today', 'sux']\n",
      "12 Today kinda sux! But I'll get by, lol 😃\n",
      "\t['12', 'today', 'kinda', 'sux', 'but', 'i', 'll', 'get', 'by', 'lol']\n",
      "sentiment analysis is shit. \n",
      "\t['sentiment', 'analysis', 'is', 'shit']\n",
      "sentiment analysis is the shit.\n",
      "\t['sentiment', 'analysis', 'is', 'the', 'shit']\n",
      "I like to hate Michael Bay films, but I couldn't fault this one\n",
      "\t['i', 'like', 'to', 'hate', 'michael', 'bay', 'film', 'but', 'i', 'couldn', 't', 'fault', 'this', 'one']\n"
     ]
    }
   ],
   "source": [
    "for text in sample_text:\n",
    "    tokens = generate_tokens(text, tokenizer=tokenizer_3, default_stopwords=default_stopwords_1, lemmatizer=lemmatizer)\n",
    "    print(f\"{text}\\n\\t{tokens}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa5800-4a2c-43d0-b485-a304dc653d70",
   "metadata": {},
   "source": [
    "Lemma + stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39d309b0-ec1f-4d05-b398-4a7c6aa97c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokens of sample_text for testing\n",
    "docs = [\n",
    "    generate_tokens(doc, tokenizer=tokenizer_3, lemmatizer=lemmatizer, default_stopwords=default_stopwords_1, min_token_length=3)\n",
    "    for doc in sample_text]\n",
    "\n",
    "\n",
    "def generate_ngrams(selectedTokens, bigram, trigram):     \n",
    "    return trigram[bigram[selectedTokens]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76010fc4-8fe0-4d82-bb93-d4b9a95ec767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTokensOrLoad(name, df):\n",
    "    if os.path.isfile(f\"data/{name}_tokens.pickle\"):\n",
    "        # loading\n",
    "        print(\"Reading pickle file\", end=\" ... \")\n",
    "        return pd.read_pickle(f\"data/{name}_tokens.pickle\")\n",
    "    else:\n",
    "        # loading\n",
    "        print(\"Generating Tokens\", end=\" ... \")\n",
    "        tokens = df[name].apply(lambda t: generate_tokens(t, tokenizer=tokenizer_3, lemmatizer=lemmatizer, default_stopwords=default_stopwords_1, min_token_length=3) if isinstance(t, str) else t)\n",
    "        # save as pickle for later use\n",
    "        print(\"Generating pickle file\", end=\" ... \")\n",
    "        tokens.to_pickle(f\"data/{name}_tokens.pickle\")\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b56109d-5dc5-46b8-afa8-f46a9b8b6b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tokens if vlaue is a string, otherwise put in t\n",
    "# text_tokens = df_submissions['text'].apply(lambda t: generate_tokens(t, tokenizer=tokenizer_3, lemmatizer=lemmatizer, default_stopwords=default_stopwords_1, min_token_length=3) if isinstance(t, str) else t)\n",
    "# text_tokens\n",
    "# title_tokens = df_submissions['title'].apply(lambda t: generate_tokens(t, tokenizer=tokenizer_3, lemmatizer=lemmatizer, default_stopwords=default_stopwords_1, min_token_length=3))\n",
    "# title_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c2cc524-c2e7-4ea0-bd8a-fd13ca9b99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Add the two token lists to the dataframe\n",
    "# df_submissions['text_tokens'] = genTokensOrLoad('text', df_submissions)\n",
    "# df_submissions['title_tokens'] = genTokensOrLoad('title', df_submissions)\n",
    "# df_submissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427b1f3e-3ec2-4bbf-88f6-720b001b7e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>numReplies</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>is_self</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>date_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>8zbz30</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>Putin Just Said that $400MM eluded Russian and...</td>\n",
       "      <td>Isn't that a HUGE admission and sign that he w...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.greatawakening</td>\n",
       "      <td>https://www.reddit.com/r/greatawakening/commen...</td>\n",
       "      <td>/r/greatawakening/comments/8zbz30/putin_just_s...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-16 15:46:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>8zc0k7</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>OMG! Where's the SERVER!!! CRAZY! This is happ...</td>\n",
       "      <td>OMG, OMG!  I love him so much!</td>\n",
       "      <td>True</td>\n",
       "      <td>self.greatawakening</td>\n",
       "      <td>https://www.reddit.com/r/greatawakening/commen...</td>\n",
       "      <td>/r/greatawakening/comments/8zc0k7/omg_wheres_t...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-16 15:51:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>8zc21l</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>Who is that Jackass reporter asking about \"Com...</td>\n",
       "      <td>Also the same one who asked about interference...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.greatawakening</td>\n",
       "      <td>https://www.reddit.com/r/greatawakening/commen...</td>\n",
       "      <td>/r/greatawakening/comments/8zc21l/who_is_that_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-16 15:56:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>90gl0h</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>Tucker's Final Exam 7/19 - Lights Out/Storm!</td>\n",
       "      <td>Tucker has been making little drops here and t...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.greatawakening</td>\n",
       "      <td>https://www.reddit.com/r/greatawakening/commen...</td>\n",
       "      <td>/r/greatawakening/comments/90gl0h/tuckers_fina...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-20 14:46:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>93jk5j</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>879f283b831c13474e219e88663d95b0763cca9b</td>\n",
       "      <td>Way to go PATRIOTS in Tampa!</td>\n",
       "      <td>I have chills and tears of joy watching you al...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.greatawakening</td>\n",
       "      <td>https://www.reddit.com/r/greatawakening/commen...</td>\n",
       "      <td>/r/greatawakening/comments/93jk5j/way_to_go_pa...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-07-31 23:20:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2773808</th>\n",
       "      <td>howardstern</td>\n",
       "      <td>62004j</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>c039f11ff985f7d8547748c10ccd5c8416b8404d</td>\n",
       "      <td>BABABOOEY on the wrap up show</td>\n",
       "      <td>Is talking so godamn fast with his JOHN hein c...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.howardstern</td>\n",
       "      <td>https://www.reddit.com/r/howardstern/comments/...</td>\n",
       "      <td>/r/howardstern/comments/62004j/bababooey_on_th...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2017-03-28 15:25:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774089</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>8eky0u</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>c039f11ff985f7d8547748c10ccd5c8416b8404d</td>\n",
       "      <td>IF IRAN RESTARTS THEIR NUCLEAR PROGRAM THEY WI...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>True</td>\n",
       "      <td>self.The_Donald</td>\n",
       "      <td>https://www.reddit.com/r/The_Donald/comments/8...</td>\n",
       "      <td>/r/The_Donald/comments/8eky0u/if_iran_restarts...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-04-24 14:46:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774632</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>9uoze6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>c039f11ff985f7d8547748c10ccd5c8416b8404d</td>\n",
       "      <td>WE NEED HIGH ENERGY TODAY PEDES!! NEVER FORGET!!!</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>True</td>\n",
       "      <td>self.The_Donald</td>\n",
       "      <td>https://www.reddit.com/r/The_Donald/comments/9...</td>\n",
       "      <td>/r/The_Donald/comments/9uoze6/we_need_high_ene...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2018-11-06 14:57:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774930</th>\n",
       "      <td>unpopularopinion</td>\n",
       "      <td>avxrw3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>c039f11ff985f7d8547748c10ccd5c8416b8404d</td>\n",
       "      <td>Miracle whip is much better than mayo.</td>\n",
       "      <td>I don’t know if it’s because of where I grew u...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.unpopularopinion</td>\n",
       "      <td>https://www.reddit.com/r/unpopularopinion/comm...</td>\n",
       "      <td>/r/unpopularopinion/comments/avxrw3/miracle_wh...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-02-28 23:43:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2775019</th>\n",
       "      <td>The_Donald</td>\n",
       "      <td>b6q1t8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>c039f11ff985f7d8547748c10ccd5c8416b8404d</td>\n",
       "      <td>Pencil neck schiff.</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>True</td>\n",
       "      <td>self.The_Donald</td>\n",
       "      <td>https://www.reddit.com/r/The_Donald/comments/b...</td>\n",
       "      <td>/r/The_Donald/comments/b6q1t8/pencil_neck_schiff/</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-03-28 23:33:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281159 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                subreddit      id score numReplies  \\\n",
       "6          greatawakening  8zbz30     1        169   \n",
       "7          greatawakening  8zc0k7     1         25   \n",
       "8          greatawakening  8zc21l     1         14   \n",
       "10         greatawakening  90gl0h     1          6   \n",
       "16         greatawakening  93jk5j     1          5   \n",
       "...                   ...     ...   ...        ...   \n",
       "2773808       howardstern  62004j    12          8   \n",
       "2774089        The_Donald  8eky0u    13          3   \n",
       "2774632        The_Donald  9uoze6     1          0   \n",
       "2774930  unpopularopinion  avxrw3     6          4   \n",
       "2775019        The_Donald  b6q1t8     1          0   \n",
       "\n",
       "                                           author  \\\n",
       "6        879f283b831c13474e219e88663d95b0763cca9b   \n",
       "7        879f283b831c13474e219e88663d95b0763cca9b   \n",
       "8        879f283b831c13474e219e88663d95b0763cca9b   \n",
       "10       879f283b831c13474e219e88663d95b0763cca9b   \n",
       "16       879f283b831c13474e219e88663d95b0763cca9b   \n",
       "...                                           ...   \n",
       "2773808  c039f11ff985f7d8547748c10ccd5c8416b8404d   \n",
       "2774089  c039f11ff985f7d8547748c10ccd5c8416b8404d   \n",
       "2774632  c039f11ff985f7d8547748c10ccd5c8416b8404d   \n",
       "2774930  c039f11ff985f7d8547748c10ccd5c8416b8404d   \n",
       "2775019  c039f11ff985f7d8547748c10ccd5c8416b8404d   \n",
       "\n",
       "                                                     title  \\\n",
       "6        Putin Just Said that $400MM eluded Russian and...   \n",
       "7        OMG! Where's the SERVER!!! CRAZY! This is happ...   \n",
       "8        Who is that Jackass reporter asking about \"Com...   \n",
       "10            Tucker's Final Exam 7/19 - Lights Out/Storm!   \n",
       "16                            Way to go PATRIOTS in Tampa!   \n",
       "...                                                    ...   \n",
       "2773808                      BABABOOEY on the wrap up show   \n",
       "2774089  IF IRAN RESTARTS THEIR NUCLEAR PROGRAM THEY WI...   \n",
       "2774632  WE NEED HIGH ENERGY TODAY PEDES!! NEVER FORGET!!!   \n",
       "2774930             Miracle whip is much better than mayo.   \n",
       "2775019                                Pencil neck schiff.   \n",
       "\n",
       "                                                      text is_self  \\\n",
       "6        Isn't that a HUGE admission and sign that he w...    True   \n",
       "7                           OMG, OMG!  I love him so much!    True   \n",
       "8        Also the same one who asked about interference...    True   \n",
       "10       Tucker has been making little drops here and t...    True   \n",
       "16       I have chills and tears of joy watching you al...    True   \n",
       "...                                                    ...     ...   \n",
       "2773808  Is talking so godamn fast with his JOHN hein c...    True   \n",
       "2774089                                          [removed]    True   \n",
       "2774632                                          [removed]    True   \n",
       "2774930  I don’t know if it’s because of where I grew u...    True   \n",
       "2775019                                          [removed]    True   \n",
       "\n",
       "                        domain  \\\n",
       "6          self.greatawakening   \n",
       "7          self.greatawakening   \n",
       "8          self.greatawakening   \n",
       "10         self.greatawakening   \n",
       "16         self.greatawakening   \n",
       "...                        ...   \n",
       "2773808       self.howardstern   \n",
       "2774089        self.The_Donald   \n",
       "2774632        self.The_Donald   \n",
       "2774930  self.unpopularopinion   \n",
       "2775019        self.The_Donald   \n",
       "\n",
       "                                                       url  \\\n",
       "6        https://www.reddit.com/r/greatawakening/commen...   \n",
       "7        https://www.reddit.com/r/greatawakening/commen...   \n",
       "8        https://www.reddit.com/r/greatawakening/commen...   \n",
       "10       https://www.reddit.com/r/greatawakening/commen...   \n",
       "16       https://www.reddit.com/r/greatawakening/commen...   \n",
       "...                                                    ...   \n",
       "2773808  https://www.reddit.com/r/howardstern/comments/...   \n",
       "2774089  https://www.reddit.com/r/The_Donald/comments/8...   \n",
       "2774632  https://www.reddit.com/r/The_Donald/comments/9...   \n",
       "2774930  https://www.reddit.com/r/unpopularopinion/comm...   \n",
       "2775019  https://www.reddit.com/r/The_Donald/comments/b...   \n",
       "\n",
       "                                                 permalink  upvote_ratio  \\\n",
       "6        /r/greatawakening/comments/8zbz30/putin_just_s...          -1.0   \n",
       "7        /r/greatawakening/comments/8zc0k7/omg_wheres_t...          -1.0   \n",
       "8        /r/greatawakening/comments/8zc21l/who_is_that_...          -1.0   \n",
       "10       /r/greatawakening/comments/90gl0h/tuckers_fina...          -1.0   \n",
       "16       /r/greatawakening/comments/93jk5j/way_to_go_pa...          -1.0   \n",
       "...                                                    ...           ...   \n",
       "2773808  /r/howardstern/comments/62004j/bababooey_on_th...          -1.0   \n",
       "2774089  /r/The_Donald/comments/8eky0u/if_iran_restarts...          -1.0   \n",
       "2774632  /r/The_Donald/comments/9uoze6/we_need_high_ene...          -1.0   \n",
       "2774930  /r/unpopularopinion/comments/avxrw3/miracle_wh...          -1.0   \n",
       "2775019  /r/The_Donald/comments/b6q1t8/pencil_neck_schiff/          -1.0   \n",
       "\n",
       "               date_created  \n",
       "6       2018-07-16 15:46:51  \n",
       "7       2018-07-16 15:51:42  \n",
       "8       2018-07-16 15:56:37  \n",
       "10      2018-07-20 14:46:01  \n",
       "16      2018-07-31 23:20:55  \n",
       "...                     ...  \n",
       "2773808 2017-03-28 15:25:29  \n",
       "2774089 2018-04-24 14:46:14  \n",
       "2774632 2018-11-06 14:57:01  \n",
       "2774930 2019-02-28 23:43:24  \n",
       "2775019 2019-03-28 23:33:18  \n",
       "\n",
       "[281159 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submissions.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171628ec-2228-4218-a3b0-29bf43d40e6c",
   "metadata": {},
   "source": [
    "## Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "612032fb-1cd5-403c-8f23-7a36ead2addf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>numReplies</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>is_self</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>date_created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259450</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>c2bbpp</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ce90110e1a553f8e6c03aed8d5183b5303b3721e</td>\n",
       "      <td>hacks and leaks rough timeline</td>\n",
       "      <td>\\n\\n{I've been trying to read up on the claim...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/c...</td>\n",
       "      <td>/r/TruthLeaks/comments/c2bbpp/hacks_and_leaks_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-06-19 02:24:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259454</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>c2tbsn</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ce90110e1a553f8e6c03aed8d5183b5303b3721e</td>\n",
       "      <td>Catholic Relief Services (1985) involved in di...</td>\n",
       "      <td>pg1\\n\\n[https://web.archive.org/web/2017112119...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/c...</td>\n",
       "      <td>/r/TruthLeaks/comments/c2tbsn/catholic_relief_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-06-20 08:10:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260132</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>he84kn</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ce90110e1a553f8e6c03aed8d5183b5303b3721e</td>\n",
       "      <td>Steve Bing to Pay $200K for Clinton Korea Trip...</td>\n",
       "      <td>\\n\\n# Steve Bing to Pay $200K for Clinton Kor...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/h...</td>\n",
       "      <td>/r/TruthLeaks/comments/he84kn/steve_bing_to_pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-23 04:42:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277659</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>6chtnt</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>017d9eab0596673d332fd9d916c3775deb4568e1</td>\n",
       "      <td>Mainlining Money - Major Southern California N...</td>\n",
       "      <td>May 21 2017 Orange County Register published t...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/6...</td>\n",
       "      <td>/r/TruthLeaks/comments/6chtnt/mainlining_money...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2017-05-21 17:40:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277664</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>6d007z</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>017d9eab0596673d332fd9d916c3775deb4568e1</td>\n",
       "      <td>How Ironic - Podesta Received an Email From Da...</td>\n",
       "      <td>Here's WL Podesta email he received from Daily...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/6...</td>\n",
       "      <td>/r/TruthLeaks/comments/6d007z/how_ironic_podes...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2017-05-24 04:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539097</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>j5k1jo</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>The Left's Scaredy-Karen Response to COVID, Sh...</td>\n",
       "      <td>They'll never make the connection, so we have ...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/j...</td>\n",
       "      <td>/r/TruthLeaks/comments/j5k1jo/the_lefts_scared...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-10-05 14:08:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539098</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>j84m0e</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>Stop Posting \"Different types of aliens\" on ea...</td>\n",
       "      <td>This message is for that one group of people t...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/j...</td>\n",
       "      <td>/r/TruthLeaks/comments/j84m0e/stop_posting_dif...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-10-09 18:37:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539099</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>j84oqa</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>Let's Get Real Here: If the \"Fly\" Won the VP D...</td>\n",
       "      <td>Change my mind\\n\\nEttiquete Robot vs Phoney De...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/j...</td>\n",
       "      <td>/r/TruthLeaks/comments/j84oqa/lets_get_real_he...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-10-09 18:41:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539122</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>kpdboe</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>Neighborhood News Studio on Bitchute -- George...</td>\n",
       "      <td>Bitchute NNS Channel\\n\\nhttps://www.bitch  yut...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/k...</td>\n",
       "      <td>/r/TruthLeaks/comments/kpdboe/neighborhood_new...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-01-03 04:09:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539123</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>kpddkq</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>Bye! I'm leaving this sub as mod. Have fun. Fi...</td>\n",
       "      <td>Thanks everyone for your efforts\\n\\nnew year o...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/k...</td>\n",
       "      <td>/r/TruthLeaks/comments/kpddkq/bye_im_leaving_t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-01-03 04:12:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit      id score numReplies  \\\n",
       "259450   TruthLeaks  c2bbpp     2          0   \n",
       "259454   TruthLeaks  c2tbsn     3          3   \n",
       "260132   TruthLeaks  he84kn     1          2   \n",
       "277659   TruthLeaks  6chtnt     4          0   \n",
       "277664   TruthLeaks  6d007z     3          0   \n",
       "...             ...     ...   ...        ...   \n",
       "2539097  TruthLeaks  j5k1jo     1          2   \n",
       "2539098  TruthLeaks  j84m0e     1          0   \n",
       "2539099  TruthLeaks  j84oqa     1          3   \n",
       "2539122  TruthLeaks  kpdboe     1          0   \n",
       "2539123  TruthLeaks  kpddkq     1          0   \n",
       "\n",
       "                                           author  \\\n",
       "259450   ce90110e1a553f8e6c03aed8d5183b5303b3721e   \n",
       "259454   ce90110e1a553f8e6c03aed8d5183b5303b3721e   \n",
       "260132   ce90110e1a553f8e6c03aed8d5183b5303b3721e   \n",
       "277659   017d9eab0596673d332fd9d916c3775deb4568e1   \n",
       "277664   017d9eab0596673d332fd9d916c3775deb4568e1   \n",
       "...                                           ...   \n",
       "2539097  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "2539098  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "2539099  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "2539122  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "2539123  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "\n",
       "                                                     title  \\\n",
       "259450                      hacks and leaks rough timeline   \n",
       "259454   Catholic Relief Services (1985) involved in di...   \n",
       "260132   Steve Bing to Pay $200K for Clinton Korea Trip...   \n",
       "277659   Mainlining Money - Major Southern California N...   \n",
       "277664   How Ironic - Podesta Received an Email From Da...   \n",
       "...                                                    ...   \n",
       "2539097  The Left's Scaredy-Karen Response to COVID, Sh...   \n",
       "2539098  Stop Posting \"Different types of aliens\" on ea...   \n",
       "2539099  Let's Get Real Here: If the \"Fly\" Won the VP D...   \n",
       "2539122  Neighborhood News Studio on Bitchute -- George...   \n",
       "2539123  Bye! I'm leaving this sub as mod. Have fun. Fi...   \n",
       "\n",
       "                                                      text is_self  \\\n",
       "259450    \\n\\n{I've been trying to read up on the claim...    True   \n",
       "259454   pg1\\n\\n[https://web.archive.org/web/2017112119...    True   \n",
       "260132    \\n\\n# Steve Bing to Pay $200K for Clinton Kor...    True   \n",
       "277659   May 21 2017 Orange County Register published t...    True   \n",
       "277664   Here's WL Podesta email he received from Daily...    True   \n",
       "...                                                    ...     ...   \n",
       "2539097  They'll never make the connection, so we have ...    True   \n",
       "2539098  This message is for that one group of people t...    True   \n",
       "2539099  Change my mind\\n\\nEttiquete Robot vs Phoney De...    True   \n",
       "2539122  Bitchute NNS Channel\\n\\nhttps://www.bitch  yut...    True   \n",
       "2539123  Thanks everyone for your efforts\\n\\nnew year o...    True   \n",
       "\n",
       "                  domain                                                url  \\\n",
       "259450   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/c...   \n",
       "259454   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/c...   \n",
       "260132   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/h...   \n",
       "277659   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/6...   \n",
       "277664   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/6...   \n",
       "...                  ...                                                ...   \n",
       "2539097  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/j...   \n",
       "2539098  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/j...   \n",
       "2539099  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/j...   \n",
       "2539122  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/k...   \n",
       "2539123  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/k...   \n",
       "\n",
       "                                                 permalink  upvote_ratio  \\\n",
       "259450   /r/TruthLeaks/comments/c2bbpp/hacks_and_leaks_...          -1.0   \n",
       "259454   /r/TruthLeaks/comments/c2tbsn/catholic_relief_...          -1.0   \n",
       "260132   /r/TruthLeaks/comments/he84kn/steve_bing_to_pa...           1.0   \n",
       "277659   /r/TruthLeaks/comments/6chtnt/mainlining_money...          -1.0   \n",
       "277664   /r/TruthLeaks/comments/6d007z/how_ironic_podes...          -1.0   \n",
       "...                                                    ...           ...   \n",
       "2539097  /r/TruthLeaks/comments/j5k1jo/the_lefts_scared...           1.0   \n",
       "2539098  /r/TruthLeaks/comments/j84m0e/stop_posting_dif...           1.0   \n",
       "2539099  /r/TruthLeaks/comments/j84oqa/lets_get_real_he...           1.0   \n",
       "2539122  /r/TruthLeaks/comments/kpdboe/neighborhood_new...           1.0   \n",
       "2539123  /r/TruthLeaks/comments/kpddkq/bye_im_leaving_t...           1.0   \n",
       "\n",
       "               date_created  \n",
       "259450  2019-06-19 02:24:41  \n",
       "259454  2019-06-20 08:10:04  \n",
       "260132  2020-06-23 04:42:46  \n",
       "277659  2017-05-21 17:40:13  \n",
       "277664  2017-05-24 04:42:00  \n",
       "...                     ...  \n",
       "2539097 2020-10-05 14:08:48  \n",
       "2539098 2020-10-09 18:37:31  \n",
       "2539099 2020-10-09 18:41:38  \n",
       "2539122 2021-01-03 04:09:49  \n",
       "2539123 2021-01-03 04:12:49  \n",
       "\n",
       "[1867 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit = \"TruthLeaks\"\n",
    "# Get only submissions from given subreddit\n",
    "df = df_submissions[df_submissions['subreddit'] == subreddit]\n",
    "# Drop na in subset\n",
    "df = df.dropna(subset=['text'])\n",
    "df = df[df['text'] != df['title']]\n",
    "\n",
    "## Removed [removed] from text and title\n",
    "df = df[df['title'] != \"[removed]\"]\n",
    "df = df[df['text'] != \"[removed]\"]\n",
    "\n",
    "## Remove duplicates in text/title\n",
    "df = df.drop_duplicates(subset=['text', 'title'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7531274-2bbc-437b-8853-1b9981ad358a",
   "metadata": {},
   "source": [
    "## Topic Modellingfrom gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "459730d6-1cae-4f22-bda0-7fe297fe3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0f98f11-0e8c-483d-a200-f1649b847ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_topics(verbose=False):\n",
    "    \"\"\"Finds the best relabelling (permutation) to maxamise alignment of title_topic and text_topic.\n",
    "    \"\"\"\n",
    "\n",
    "    from itertools import permutations\n",
    "\n",
    "    data = pd.crosstab(df.title_topic, df.text_topic).values\n",
    "    n = data.shape[0]\n",
    "\n",
    "    max_score = np.finfo(float).min\n",
    "    max_permutation = None\n",
    "    for permutation in permutations(range(n)):\n",
    "        tmp =  np.array([data[k] for k in permutation])\n",
    "        score = tmp.trace()\n",
    "        if score>max_score:\n",
    "            max_score = score\n",
    "            max_permutation = permutation\n",
    "\n",
    "    if verbose:\n",
    "        mapping = \", \".join([f\"{k}->{v}\"  for k,v in enumerate(max_permutation)])\n",
    "        print(f\"Max score {max_score} obtained with relabeling:\\n\\t {mapping}\\nand resulting cross table of\\n{data}\")\n",
    "\n",
    "    return max_score, max_permutation, np.array([data[k] for k in max_permutation])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ef0b614-978f-45db-b91a-95915a705148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tokens(t):\n",
    "    return generate_tokens(t, tokenizer=tokenizer_3, lemmatizer=lemmatizer, default_stopwords=default_stopwords_1, min_token_length=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9a992-600d-4eb4-aca4-03cdf8c66853",
   "metadata": {},
   "source": [
    "Generate tokens and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e26e04b0-45d5-45fe-9662-7c44817acc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>numReplies</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>is_self</th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>date_created</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_ngrams</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>title_ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259450</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>c2bbpp</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ce90110e1a553f8e6c03aed8d5183b5303b3721e</td>\n",
       "      <td>hacks and leaks rough timeline</td>\n",
       "      <td>\\n\\n{I've been trying to read up on the claim...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/c...</td>\n",
       "      <td>/r/TruthLeaks/comments/c2bbpp/hacks_and_leaks_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-06-19 02:24:41</td>\n",
       "      <td>[been, trying, read, the, claim, about, russia...</td>\n",
       "      <td>[been, trying, read, the, claim, about, russia...</td>\n",
       "      <td>[hack, and, leak, rough, timeline]</td>\n",
       "      <td>[hack, and, leak, rough, timeline]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259454</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>c2tbsn</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>ce90110e1a553f8e6c03aed8d5183b5303b3721e</td>\n",
       "      <td>Catholic Relief Services (1985) involved in di...</td>\n",
       "      <td>pg1\\n\\n[https://web.archive.org/web/2017112119...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/c...</td>\n",
       "      <td>/r/TruthLeaks/comments/c2tbsn/catholic_relief_...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-06-20 08:10:04</td>\n",
       "      <td>[pg1, http, web, archive, org, web, 2017112119...</td>\n",
       "      <td>[pg1, http_web_archive, org_web, 2017112119482...</td>\n",
       "      <td>[catholic, relief, service, 1985, involved, di...</td>\n",
       "      <td>[catholic, relief, service, 1985, involved, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260132</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>he84kn</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ce90110e1a553f8e6c03aed8d5183b5303b3721e</td>\n",
       "      <td>Steve Bing to Pay $200K for Clinton Korea Trip...</td>\n",
       "      <td>\\n\\n# Steve Bing to Pay $200K for Clinton Kor...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/h...</td>\n",
       "      <td>/r/TruthLeaks/comments/he84kn/steve_bing_to_pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-23 04:42:46</td>\n",
       "      <td>[steve, bing, pay, $200, for, clinton, korea, ...</td>\n",
       "      <td>[steve, bing, pay, $200, for, clinton, korea, ...</td>\n",
       "      <td>[steve, bing, pay, $200, for, clinton, korea, ...</td>\n",
       "      <td>[steve, bing, pay, $200, for, clinton, korea, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277659</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>6chtnt</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>017d9eab0596673d332fd9d916c3775deb4568e1</td>\n",
       "      <td>Mainlining Money - Major Southern California N...</td>\n",
       "      <td>May 21 2017 Orange County Register published t...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/6...</td>\n",
       "      <td>/r/TruthLeaks/comments/6chtnt/mainlining_money...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2017-05-21 17:40:13</td>\n",
       "      <td>[may, 2017, orange, county, register, publishe...</td>\n",
       "      <td>[may_2017, orange, county, register, published...</td>\n",
       "      <td>[mainlining, money, major, southern, californi...</td>\n",
       "      <td>[mainlining, money, major, southern, californi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277664</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>6d007z</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>017d9eab0596673d332fd9d916c3775deb4568e1</td>\n",
       "      <td>How Ironic - Podesta Received an Email From Da...</td>\n",
       "      <td>Here's WL Podesta email he received from Daily...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/6...</td>\n",
       "      <td>/r/TruthLeaks/comments/6d007z/how_ironic_podes...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2017-05-24 04:42:00</td>\n",
       "      <td>[here, podesta, email, received, from, daily, ...</td>\n",
       "      <td>[here, podesta_email, received, from, daily_ne...</td>\n",
       "      <td>[how, ironic, podesta, received, email, from, ...</td>\n",
       "      <td>[how, ironic, podesta, received, email, from, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539097</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>j5k1jo</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>The Left's Scaredy-Karen Response to COVID, Sh...</td>\n",
       "      <td>They'll never make the connection, so we have ...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/j...</td>\n",
       "      <td>/r/TruthLeaks/comments/j5k1jo/the_lefts_scared...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-10-05 14:08:48</td>\n",
       "      <td>[they, never, make, the, connection, have, mak...</td>\n",
       "      <td>[they, never, make, the, connection, have, mak...</td>\n",
       "      <td>[the, left, scaredy, karen, response, covid, s...</td>\n",
       "      <td>[the_left, scaredy, karen, response, covid, sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539098</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>j84m0e</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>Stop Posting \"Different types of aliens\" on ea...</td>\n",
       "      <td>This message is for that one group of people t...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/j...</td>\n",
       "      <td>/r/TruthLeaks/comments/j84m0e/stop_posting_dif...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-10-09 18:37:31</td>\n",
       "      <td>[this, message, for, that, one, group, people,...</td>\n",
       "      <td>[this, message, for, that, one, group, people,...</td>\n",
       "      <td>[stop, posting, different, type, alien, earth,...</td>\n",
       "      <td>[stop, posting, different, type, alien, earth,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539099</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>j84oqa</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>Let's Get Real Here: If the \"Fly\" Won the VP D...</td>\n",
       "      <td>Change my mind\\n\\nEttiquete Robot vs Phoney De...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/j...</td>\n",
       "      <td>/r/TruthLeaks/comments/j84oqa/lets_get_real_he...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-10-09 18:41:38</td>\n",
       "      <td>[change, mind, ettiquete, robot, phoney, demon...</td>\n",
       "      <td>[change, mind, ettiquete, robot, phoney, demon...</td>\n",
       "      <td>[let, get, real, here, the, fly, won, the, deb...</td>\n",
       "      <td>[let, get, real, here, the, fly, won, the, deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539122</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>kpdboe</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>Neighborhood News Studio on Bitchute -- George...</td>\n",
       "      <td>Bitchute NNS Channel\\n\\nhttps://www.bitch  yut...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/k...</td>\n",
       "      <td>/r/TruthLeaks/comments/kpdboe/neighborhood_new...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-01-03 04:09:49</td>\n",
       "      <td>[bitchute, nns, channel, http, www, bitch, yut...</td>\n",
       "      <td>[bitchute, nns, channel, http_www, bitch, yute...</td>\n",
       "      <td>[neighborhood, news, studio, bitchute, george,...</td>\n",
       "      <td>[neighborhood, news, studio, bitchute, george_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539123</th>\n",
       "      <td>TruthLeaks</td>\n",
       "      <td>kpddkq</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15ab943a0e8ec63f3b75e841a35ae082b5878521</td>\n",
       "      <td>Bye! I'm leaving this sub as mod. Have fun. Fi...</td>\n",
       "      <td>Thanks everyone for your efforts\\n\\nnew year o...</td>\n",
       "      <td>True</td>\n",
       "      <td>self.TruthLeaks</td>\n",
       "      <td>https://www.reddit.com/r/TruthLeaks/comments/k...</td>\n",
       "      <td>/r/TruthLeaks/comments/kpddkq/bye_im_leaving_t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-01-03 04:12:49</td>\n",
       "      <td>[thanks, everyone, for, your, effort, new, yea...</td>\n",
       "      <td>[thanks_everyone, for, your, effort, new, year...</td>\n",
       "      <td>[bye, leaving, this, sub, mod, have, fun, find...</td>\n",
       "      <td>[bye, leaving, this_sub, mod, have, fun, find,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit      id score numReplies  \\\n",
       "259450   TruthLeaks  c2bbpp     2          0   \n",
       "259454   TruthLeaks  c2tbsn     3          3   \n",
       "260132   TruthLeaks  he84kn     1          2   \n",
       "277659   TruthLeaks  6chtnt     4          0   \n",
       "277664   TruthLeaks  6d007z     3          0   \n",
       "...             ...     ...   ...        ...   \n",
       "2539097  TruthLeaks  j5k1jo     1          2   \n",
       "2539098  TruthLeaks  j84m0e     1          0   \n",
       "2539099  TruthLeaks  j84oqa     1          3   \n",
       "2539122  TruthLeaks  kpdboe     1          0   \n",
       "2539123  TruthLeaks  kpddkq     1          0   \n",
       "\n",
       "                                           author  \\\n",
       "259450   ce90110e1a553f8e6c03aed8d5183b5303b3721e   \n",
       "259454   ce90110e1a553f8e6c03aed8d5183b5303b3721e   \n",
       "260132   ce90110e1a553f8e6c03aed8d5183b5303b3721e   \n",
       "277659   017d9eab0596673d332fd9d916c3775deb4568e1   \n",
       "277664   017d9eab0596673d332fd9d916c3775deb4568e1   \n",
       "...                                           ...   \n",
       "2539097  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "2539098  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "2539099  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "2539122  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "2539123  15ab943a0e8ec63f3b75e841a35ae082b5878521   \n",
       "\n",
       "                                                     title  \\\n",
       "259450                      hacks and leaks rough timeline   \n",
       "259454   Catholic Relief Services (1985) involved in di...   \n",
       "260132   Steve Bing to Pay $200K for Clinton Korea Trip...   \n",
       "277659   Mainlining Money - Major Southern California N...   \n",
       "277664   How Ironic - Podesta Received an Email From Da...   \n",
       "...                                                    ...   \n",
       "2539097  The Left's Scaredy-Karen Response to COVID, Sh...   \n",
       "2539098  Stop Posting \"Different types of aliens\" on ea...   \n",
       "2539099  Let's Get Real Here: If the \"Fly\" Won the VP D...   \n",
       "2539122  Neighborhood News Studio on Bitchute -- George...   \n",
       "2539123  Bye! I'm leaving this sub as mod. Have fun. Fi...   \n",
       "\n",
       "                                                      text is_self  \\\n",
       "259450    \\n\\n{I've been trying to read up on the claim...    True   \n",
       "259454   pg1\\n\\n[https://web.archive.org/web/2017112119...    True   \n",
       "260132    \\n\\n# Steve Bing to Pay $200K for Clinton Kor...    True   \n",
       "277659   May 21 2017 Orange County Register published t...    True   \n",
       "277664   Here's WL Podesta email he received from Daily...    True   \n",
       "...                                                    ...     ...   \n",
       "2539097  They'll never make the connection, so we have ...    True   \n",
       "2539098  This message is for that one group of people t...    True   \n",
       "2539099  Change my mind\\n\\nEttiquete Robot vs Phoney De...    True   \n",
       "2539122  Bitchute NNS Channel\\n\\nhttps://www.bitch  yut...    True   \n",
       "2539123  Thanks everyone for your efforts\\n\\nnew year o...    True   \n",
       "\n",
       "                  domain                                                url  \\\n",
       "259450   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/c...   \n",
       "259454   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/c...   \n",
       "260132   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/h...   \n",
       "277659   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/6...   \n",
       "277664   self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/6...   \n",
       "...                  ...                                                ...   \n",
       "2539097  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/j...   \n",
       "2539098  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/j...   \n",
       "2539099  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/j...   \n",
       "2539122  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/k...   \n",
       "2539123  self.TruthLeaks  https://www.reddit.com/r/TruthLeaks/comments/k...   \n",
       "\n",
       "                                                 permalink  upvote_ratio  \\\n",
       "259450   /r/TruthLeaks/comments/c2bbpp/hacks_and_leaks_...          -1.0   \n",
       "259454   /r/TruthLeaks/comments/c2tbsn/catholic_relief_...          -1.0   \n",
       "260132   /r/TruthLeaks/comments/he84kn/steve_bing_to_pa...           1.0   \n",
       "277659   /r/TruthLeaks/comments/6chtnt/mainlining_money...          -1.0   \n",
       "277664   /r/TruthLeaks/comments/6d007z/how_ironic_podes...          -1.0   \n",
       "...                                                    ...           ...   \n",
       "2539097  /r/TruthLeaks/comments/j5k1jo/the_lefts_scared...           1.0   \n",
       "2539098  /r/TruthLeaks/comments/j84m0e/stop_posting_dif...           1.0   \n",
       "2539099  /r/TruthLeaks/comments/j84oqa/lets_get_real_he...           1.0   \n",
       "2539122  /r/TruthLeaks/comments/kpdboe/neighborhood_new...           1.0   \n",
       "2539123  /r/TruthLeaks/comments/kpddkq/bye_im_leaving_t...           1.0   \n",
       "\n",
       "               date_created  \\\n",
       "259450  2019-06-19 02:24:41   \n",
       "259454  2019-06-20 08:10:04   \n",
       "260132  2020-06-23 04:42:46   \n",
       "277659  2017-05-21 17:40:13   \n",
       "277664  2017-05-24 04:42:00   \n",
       "...                     ...   \n",
       "2539097 2020-10-05 14:08:48   \n",
       "2539098 2020-10-09 18:37:31   \n",
       "2539099 2020-10-09 18:41:38   \n",
       "2539122 2021-01-03 04:09:49   \n",
       "2539123 2021-01-03 04:12:49   \n",
       "\n",
       "                                               text_tokens  \\\n",
       "259450   [been, trying, read, the, claim, about, russia...   \n",
       "259454   [pg1, http, web, archive, org, web, 2017112119...   \n",
       "260132   [steve, bing, pay, $200, for, clinton, korea, ...   \n",
       "277659   [may, 2017, orange, county, register, publishe...   \n",
       "277664   [here, podesta, email, received, from, daily, ...   \n",
       "...                                                    ...   \n",
       "2539097  [they, never, make, the, connection, have, mak...   \n",
       "2539098  [this, message, for, that, one, group, people,...   \n",
       "2539099  [change, mind, ettiquete, robot, phoney, demon...   \n",
       "2539122  [bitchute, nns, channel, http, www, bitch, yut...   \n",
       "2539123  [thanks, everyone, for, your, effort, new, yea...   \n",
       "\n",
       "                                               text_ngrams  \\\n",
       "259450   [been, trying, read, the, claim, about, russia...   \n",
       "259454   [pg1, http_web_archive, org_web, 2017112119482...   \n",
       "260132   [steve, bing, pay, $200, for, clinton, korea, ...   \n",
       "277659   [may_2017, orange, county, register, published...   \n",
       "277664   [here, podesta_email, received, from, daily_ne...   \n",
       "...                                                    ...   \n",
       "2539097  [they, never, make, the, connection, have, mak...   \n",
       "2539098  [this, message, for, that, one, group, people,...   \n",
       "2539099  [change, mind, ettiquete, robot, phoney, demon...   \n",
       "2539122  [bitchute, nns, channel, http_www, bitch, yute...   \n",
       "2539123  [thanks_everyone, for, your, effort, new, year...   \n",
       "\n",
       "                                              title_tokens  \\\n",
       "259450                  [hack, and, leak, rough, timeline]   \n",
       "259454   [catholic, relief, service, 1985, involved, di...   \n",
       "260132   [steve, bing, pay, $200, for, clinton, korea, ...   \n",
       "277659   [mainlining, money, major, southern, californi...   \n",
       "277664   [how, ironic, podesta, received, email, from, ...   \n",
       "...                                                    ...   \n",
       "2539097  [the, left, scaredy, karen, response, covid, s...   \n",
       "2539098  [stop, posting, different, type, alien, earth,...   \n",
       "2539099  [let, get, real, here, the, fly, won, the, deb...   \n",
       "2539122  [neighborhood, news, studio, bitchute, george,...   \n",
       "2539123  [bye, leaving, this, sub, mod, have, fun, find...   \n",
       "\n",
       "                                              title_ngrams  \n",
       "259450                  [hack, and, leak, rough, timeline]  \n",
       "259454   [catholic, relief, service, 1985, involved, di...  \n",
       "260132   [steve, bing, pay, $200, for, clinton, korea, ...  \n",
       "277659   [mainlining, money, major, southern, californi...  \n",
       "277664   [how, ironic, podesta, received, email, from, ...  \n",
       "...                                                    ...  \n",
       "2539097  [the_left, scaredy, karen, response, covid, sh...  \n",
       "2539098  [stop, posting, different, type, alien, earth,...  \n",
       "2539099  [let, get, real, here, the, fly, won, the, deb...  \n",
       "2539122  [neighborhood, news, studio, bitchute, george_...  \n",
       "2539123  [bye, leaving, this_sub, mod, have, fun, find,...  \n",
       "\n",
       "[1867 rows x 17 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text\n",
    "df['text_tokens'] = df['text'].apply(gen_tokens)\n",
    "\n",
    "bigram = Phrases(df['text_tokens'], min_count=10)\n",
    "trigram = Phrases(bigram[df['text_tokens']])\n",
    "df['text_ngrams'] = df['text_tokens'].apply(lambda t: generate_ngrams(t, bigram, trigram))\n",
    "\n",
    "# Title\n",
    "df['title_tokens'] = df['title'].apply(gen_tokens)\n",
    "\n",
    "bigram = Phrases(df['title_tokens'], min_count=10)\n",
    "trigram = Phrases(bigram[df['title_tokens']])\n",
    "df['title_ngrams'] = df['title_tokens'].apply(lambda t: generate_ngrams(t, bigram, trigram))\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c4a3845-d77d-4f7b-a1d7-a7c1645aebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_LDAModel(docs, num_topics=5, chunkize=2000, passes=20, iterations=400, eval_every=1):\n",
    "    dictionary = Dictionary(docs)\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    dictionary[0]\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize,\n",
    "        alpha='auto', eta='auto',\n",
    "        iterations=iterations, num_topics=num_topics,\n",
    "        passes=passes, eval_every=eval_every)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=model, texts=docs, dictionary=dictionary,coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('Coherence Score: ', coherence_lda)\n",
    "    \n",
    "    def get_topic(doc):\n",
    "        bow = dictionary.doc2bow(doc)               # convert tokens to bow\n",
    "        topics = model.get_document_topics(bow)     # get topic,prob based on bow\n",
    "        return np.array(topics).argmax(axis=0)[1]   # pick topic with highest probability\n",
    "\n",
    "    return docs.apply(get_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1912a20a-ba1f-47ae-ad01-36a058f31b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title Topic\n",
      "Coherence Score:  0.5401265280133998\n",
      "\n",
      "Text Topic\n",
      "Coherence Score:  0.3008391294331425\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTitle Topic\")\n",
    "df['title_topic'] = generate_LDAModel(df['title_ngrams'])\n",
    "print(\"\\nText Topic\")\n",
    "df['text_topic'] = generate_LDAModel(df['text_ngrams'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b692312-f82f-4b4f-bcd9-2e5b807c6650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max score 548 obtained with relabeling:\n",
      "\t 0->0, 1->1, 2->2, 3->3, 4->4\n",
      "and resulting cross table of\n",
      "[[303  71  14   0  33]\n",
      " [278 188  54   3  23]\n",
      " [165 121  55   6   0]\n",
      " [134  88  21   0   0]\n",
      " [165 106  36   1   2]]\n"
     ]
    }
   ],
   "source": [
    "score, permutation, data = align_topics(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b86a8-6d0b-42dc-8a4c-0ffc803d8a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
